{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "from alpha_vantage.timeseries import TimeSeries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ts = TimeSeries(key='8VHYYYZR0BTCNJ2F', output_format='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import datetime\n",
    "sns.set()\n",
    "tf.compat.v1.random.set_random_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>4. close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-03-13</th>\n",
       "      <td>317.9278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-12</th>\n",
       "      <td>315.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-11</th>\n",
       "      <td>349.9200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-10</th>\n",
       "      <td>364.1300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-09</th>\n",
       "      <td>346.4900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-06</th>\n",
       "      <td>368.9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-05</th>\n",
       "      <td>372.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-04</th>\n",
       "      <td>383.7900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-03</th>\n",
       "      <td>368.7700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-02</th>\n",
       "      <td>381.0500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-28</th>\n",
       "      <td>369.0300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-27</th>\n",
       "      <td>371.7100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-26</th>\n",
       "      <td>379.2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-25</th>\n",
       "      <td>360.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-24</th>\n",
       "      <td>368.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-21</th>\n",
       "      <td>380.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-20</th>\n",
       "      <td>386.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-19</th>\n",
       "      <td>386.1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-18</th>\n",
       "      <td>387.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-14</th>\n",
       "      <td>380.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-13</th>\n",
       "      <td>381.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-12</th>\n",
       "      <td>380.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-11</th>\n",
       "      <td>373.6900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-10</th>\n",
       "      <td>371.0700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-07</th>\n",
       "      <td>366.7700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-06</th>\n",
       "      <td>366.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-05</th>\n",
       "      <td>369.6700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-04</th>\n",
       "      <td>369.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-03</th>\n",
       "      <td>358.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-31</th>\n",
       "      <td>345.0900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-25</th>\n",
       "      <td>368.3300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-24</th>\n",
       "      <td>374.2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-23</th>\n",
       "      <td>381.8900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-22</th>\n",
       "      <td>377.3400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-18</th>\n",
       "      <td>360.3500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-17</th>\n",
       "      <td>354.7400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-16</th>\n",
       "      <td>359.4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>348.8700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-12</th>\n",
       "      <td>351.1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-11</th>\n",
       "      <td>367.6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-10</th>\n",
       "      <td>363.9200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-09</th>\n",
       "      <td>364.7100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>361.4100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>365.4900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-04</th>\n",
       "      <td>367.8800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-03</th>\n",
       "      <td>369.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-02</th>\n",
       "      <td>367.7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>366.9600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-29</th>\n",
       "      <td>356.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-28</th>\n",
       "      <td>354.6100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-27</th>\n",
       "      <td>353.3700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-26</th>\n",
       "      <td>359.9700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-25</th>\n",
       "      <td>366.2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-22</th>\n",
       "      <td>361.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>377.8700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-20</th>\n",
       "      <td>375.2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-19</th>\n",
       "      <td>358.7800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-18</th>\n",
       "      <td>363.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-15</th>\n",
       "      <td>361.4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-14</th>\n",
       "      <td>358.8200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            4. close\n",
       "date                \n",
       "2020-03-13  317.9278\n",
       "2020-03-12  315.2500\n",
       "2020-03-11  349.9200\n",
       "2020-03-10  364.1300\n",
       "2020-03-09  346.4900\n",
       "...              ...\n",
       "2019-03-20  375.2200\n",
       "2019-03-19  358.7800\n",
       "2019-03-18  363.4400\n",
       "2019-03-15  361.4600\n",
       "2019-03-14  358.8200\n",
       "\n",
       "[253 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_csv('../dataset/GOOG-year.csv')\n",
    "#df.head()\n",
    "df,_ = ts.get_daily(symbol='NFLX', outputsize='full')\n",
    "df.iloc[0:253, 3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler().fit(df.iloc[0:253, 3:4].astype('float32')) # Close index\n",
    "df_log = minmax.transform(df.iloc[0:253, 3:4].astype('float32')) # Close index\n",
    "df_log = pd.DataFrame(df_log)\n",
    "df = df.iloc[0:253, 3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(df)\n",
    "np.save('nflx.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253, 1), (233, 1), (20, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 20\n",
    "simulation_size = 5\n",
    "\n",
    "df_train = df_log.iloc[:-test_size]\n",
    "df_test = df_log.iloc[-test_size:]\n",
    "df.shape, df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root Mean Square Percentage Error\n",
    "def calculate_accuracy(real, predict):\n",
    "    #print(real)\n",
    "    real = np.array(real) + 1\n",
    "    predict = np.array(predict) + 1\n",
    "    #print(np.mean(np.square((real - predict) / real)))\n",
    "    percentage = 1 - np.sqrt(np.mean(np.square((real - predict) / real)))\n",
    "    return percentage * 100\n",
    "\n",
    "def anchor(signal, weight):\n",
    "    buffer = []\n",
    "    last = signal[0]\n",
    "    for i in signal:\n",
    "        smoothed_val = last * weight + (1 - weight) * i\n",
    "        buffer.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 1\n",
    "size_layer = 128\n",
    "timestamp = 5\n",
    "epoch = 300\n",
    "dropout_rate = 0.8\n",
    "future_day = test_size\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0313 10:31:24.030882 139875228833600 deprecation.py:323] From <ipython-input-10-591827d2a9de>:12: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0313 10:31:24.033543 139875228833600 rnn_cell_impl.py:893] <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f360a529080>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n",
      "W0313 10:31:24.034771 139875228833600 deprecation.py:323] From <ipython-input-10-591827d2a9de>:16: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0313 10:31:24.633538 139875228833600 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0313 10:31:24.638367 139875228833600 deprecation.py:323] From <ipython-input-10-591827d2a9de>:27: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0313 10:31:24.870854 139875228833600 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0313 10:31:24.878933 139875228833600 deprecation.py:506] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0313 10:31:25.430973 139875228833600 deprecation.py:323] From <ipython-input-10-591827d2a9de>:29: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0313 10:31:25.628624 139875228833600 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:21.891629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0313 10:34:49.169675 139875228833600 rnn_cell_impl.py:893] <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f367989e6a0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation 2\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = LSTM(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer: init_value,\n",
    "                },\n",
    "            )        \n",
    "            init_value = last_state\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value = last_state\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    results.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc1 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results]\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_bid:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        backward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        forward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            backward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        forward_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            forward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.backward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        self.forward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_backward,\n",
    "            drop_backward,\n",
    "            self.X,\n",
    "            initial_state_fw = self.forward_hidden_layer,\n",
    "            initial_state_bw = self.backward_hidden_layer,\n",
    "            dtype = tf.float32,\n",
    "        )\n",
    "        self.outputs = tf.concat(self.outputs, 2)\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = LSTM_bid(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value_forward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.backward_hidden_layer: init_value_backward,\n",
    "                    modelnn.forward_hidden_layer: init_value_forward,\n",
    "                },\n",
    "            )        \n",
    "            init_value_forward = last_state[0]\n",
    "            init_value_backward = last_state[1]\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value_forward = last_state[0]\n",
    "    init_value_backward = last_state[1]\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results2.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results2]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results2):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc2)))\n",
    "plt.show()\n",
    "acc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_2p:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "        \n",
    "        with tf.variable_scope('forward', reuse = False):\n",
    "            rnn_cells_forward = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "                state_is_tuple = False,\n",
    "            )\n",
    "            self.X_forward = tf.placeholder(tf.float32, (None, None, size))\n",
    "            drop_forward = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_forward, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.hidden_layer_forward = tf.placeholder(\n",
    "                tf.float32, (None, num_layers * 2 * size_layer)\n",
    "            )\n",
    "            self.outputs_forward, self.last_state_forward = tf.nn.dynamic_rnn(\n",
    "                drop_forward,\n",
    "                self.X_forward,\n",
    "                initial_state = self.hidden_layer_forward,\n",
    "                dtype = tf.float32,\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('backward', reuse = False):\n",
    "            rnn_cells_backward = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "                state_is_tuple = False,\n",
    "            )\n",
    "            self.X_backward = tf.placeholder(tf.float32, (None, None, size))\n",
    "            drop_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_backward, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.hidden_layer_backward = tf.placeholder(\n",
    "                tf.float32, (None, num_layers * 2 * size_layer)\n",
    "            )\n",
    "            self.outputs_backward, self.last_state_backward = tf.nn.dynamic_rnn(\n",
    "                drop_backward,\n",
    "                self.X_backward,\n",
    "                initial_state = self.hidden_layer_backward,\n",
    "                dtype = tf.float32,\n",
    "            )\n",
    "\n",
    "        self.outputs = self.outputs_backward - self.outputs_forward\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = LSTM_2p(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value_forward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x_forward = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_x_backward = np.expand_dims(\n",
    "                np.flip(df_train.iloc[k : index, :].values, axis = 0), axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state_forward, last_state_backward, _, loss = sess.run(\n",
    "                [\n",
    "                    modelnn.logits,\n",
    "                    modelnn.last_state_forward,\n",
    "                    modelnn.last_state_backward,\n",
    "                    modelnn.optimizer,\n",
    "                    modelnn.cost,\n",
    "                ],\n",
    "                feed_dict = {\n",
    "                    modelnn.X_forward: batch_x_forward,\n",
    "                    modelnn.X_backward: batch_x_backward,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer_forward: init_value_forward,\n",
    "                    modelnn.hidden_layer_backward: init_value_backward,\n",
    "                },\n",
    "            )\n",
    "            init_value_forward = last_state_forward\n",
    "            init_value_backward = last_state_backward\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        batch_x_forward = np.expand_dims(\n",
    "        df_train.iloc[k : k + timestamp, :], axis = 0\n",
    "        )\n",
    "        batch_x_backward = np.expand_dims(\n",
    "            np.flip(df_train.iloc[k : k + timestamp, :].values, axis = 0), axis = 0\n",
    "        )\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [\n",
    "                modelnn.logits,\n",
    "                modelnn.last_state_forward,\n",
    "                modelnn.last_state_backward,\n",
    "            ],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: batch_x_forward,\n",
    "                modelnn.X_backward: batch_x_backward,\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[k + 1 : k + timestamp + 1, :] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        batch_x_forward = np.expand_dims(df_train.iloc[upper_b:, :], axis = 0)\n",
    "        batch_x_backward = np.expand_dims(\n",
    "            np.flip(df_train.iloc[upper_b:, :].values, axis = 0), axis = 0\n",
    "        )\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state_forward, modelnn.last_state_backward],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: batch_x_forward,\n",
    "                modelnn.X_backward: batch_x_backward,\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "        \n",
    "    init_value_forward = last_state_forward\n",
    "    init_value_backward = last_state_backward\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        o_f = np.flip(o, axis = 0)\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [\n",
    "                modelnn.logits,\n",
    "                modelnn.last_state_forward,\n",
    "                modelnn.last_state_backward,\n",
    "            ],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: np.expand_dims(o, axis = 0),\n",
    "                modelnn.X_backward: np.expand_dims(o_f, axis = 0),\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results3.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc3 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results3]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results3):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.GRUCell(size_layer)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results4 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = GRU(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer: init_value,\n",
    "                },\n",
    "            )        \n",
    "            init_value = last_state\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value = last_state\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results4.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc4 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results4]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results4):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc4)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_bid:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.GRUCell(size_layer)\n",
    "\n",
    "        backward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        forward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            backward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        forward_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            forward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.backward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * size_layer)\n",
    "        )\n",
    "        self.forward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_backward,\n",
    "            drop_backward,\n",
    "            self.X,\n",
    "            initial_state_fw = self.forward_hidden_layer,\n",
    "            initial_state_bw = self.backward_hidden_layer,\n",
    "            dtype = tf.float32,\n",
    "        )\n",
    "        self.outputs = tf.concat(self.outputs, 2)\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results5 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = GRU_bid(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.backward_hidden_layer: init_value_backward,\n",
    "                    modelnn.forward_hidden_layer: init_value_forward,\n",
    "                },\n",
    "            )        \n",
    "            init_value_forward = last_state[0]\n",
    "            init_value_backward = last_state[1]\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value_forward = last_state[0]\n",
    "    init_value_backward = last_state[1]\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results5.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc5 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results5]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results5):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc5)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_2p:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.GRUCell(size_layer)\n",
    "        \n",
    "        with tf.variable_scope('forward', reuse = False):\n",
    "            rnn_cells_forward = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "                state_is_tuple = False,\n",
    "            )\n",
    "            self.X_forward = tf.placeholder(tf.float32, (None, None, size))\n",
    "            drop_forward = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_forward, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.hidden_layer_forward = tf.placeholder(\n",
    "                tf.float32, (None, num_layers * size_layer)\n",
    "            )\n",
    "            self.outputs_forward, self.last_state_forward = tf.nn.dynamic_rnn(\n",
    "                drop_forward,\n",
    "                self.X_forward,\n",
    "                initial_state = self.hidden_layer_forward,\n",
    "                dtype = tf.float32,\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('backward', reuse = False):\n",
    "            rnn_cells_backward = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "                state_is_tuple = False,\n",
    "            )\n",
    "            self.X_backward = tf.placeholder(tf.float32, (None, None, size))\n",
    "            drop_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_backward, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.hidden_layer_backward = tf.placeholder(\n",
    "                tf.float32, (None, num_layers * size_layer)\n",
    "            )\n",
    "            self.outputs_backward, self.last_state_backward = tf.nn.dynamic_rnn(\n",
    "                drop_backward,\n",
    "                self.X_backward,\n",
    "                initial_state = self.hidden_layer_backward,\n",
    "                dtype = tf.float32,\n",
    "            )\n",
    "\n",
    "        self.outputs = self.outputs_backward - self.outputs_forward\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = GRU_2p(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x_forward = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_x_backward = np.expand_dims(\n",
    "                np.flip(df_train.iloc[k : index, :].values, axis = 0), axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state_forward, last_state_backward, _, loss = sess.run(\n",
    "                [\n",
    "                    modelnn.logits,\n",
    "                    modelnn.last_state_forward,\n",
    "                    modelnn.last_state_backward,\n",
    "                    modelnn.optimizer,\n",
    "                    modelnn.cost,\n",
    "                ],\n",
    "                feed_dict = {\n",
    "                    modelnn.X_forward: batch_x_forward,\n",
    "                    modelnn.X_backward: batch_x_backward,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer_forward: init_value_forward,\n",
    "                    modelnn.hidden_layer_backward: init_value_backward,\n",
    "                },\n",
    "            )\n",
    "            init_value_forward = last_state_forward\n",
    "            init_value_backward = last_state_backward\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        batch_x_forward = np.expand_dims(\n",
    "        df_train.iloc[k : k + timestamp, :], axis = 0\n",
    "        )\n",
    "        batch_x_backward = np.expand_dims(\n",
    "            np.flip(df_train.iloc[k : k + timestamp, :].values, axis = 0), axis = 0\n",
    "        )\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [\n",
    "                modelnn.logits,\n",
    "                modelnn.last_state_forward,\n",
    "                modelnn.last_state_backward,\n",
    "            ],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: batch_x_forward,\n",
    "                modelnn.X_backward: batch_x_backward,\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[k + 1 : k + timestamp + 1, :] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        batch_x_forward = np.expand_dims(df_train.iloc[upper_b:, :], axis = 0)\n",
    "        batch_x_backward = np.expand_dims(\n",
    "            np.flip(df_train.iloc[upper_b:, :].values, axis = 0), axis = 0\n",
    "        )\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state_forward, modelnn.last_state_backward],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: batch_x_forward,\n",
    "                modelnn.X_backward: batch_x_backward,\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "        \n",
    "    init_value_forward = last_state_forward\n",
    "    init_value_backward = last_state_backward\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        o_f = np.flip(o, axis = 0)\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [\n",
    "                modelnn.logits,\n",
    "                modelnn.last_state_forward,\n",
    "                modelnn.last_state_backward,\n",
    "            ],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: np.expand_dims(o, axis = 0),\n",
    "                modelnn.X_backward: np.expand_dims(o_f, axis = 0),\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results6.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc6 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results6]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results6):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc6)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('goog_gru.npy', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.BasicRNNCell(size_layer)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results7 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = VanillaRNN(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer: init_value,\n",
    "                },\n",
    "            )        \n",
    "            init_value = last_state\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value = last_state\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results7.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc7 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results7]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results7):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc7)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN_bid:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.BasicRNNCell(size_layer)\n",
    "\n",
    "        backward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        forward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            backward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        forward_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            forward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.backward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * size_layer)\n",
    "        )\n",
    "        self.forward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * size_layer)\n",
    "        )\n",
    "        self.outputs, self.last_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_backward,\n",
    "            drop_backward,\n",
    "            self.X,\n",
    "            initial_state_fw = self.forward_hidden_layer,\n",
    "            initial_state_bw = self.backward_hidden_layer,\n",
    "            dtype = tf.float32,\n",
    "        )\n",
    "        self.outputs = tf.concat(self.outputs, 2)\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results8 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = VanillaRNN_bid(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.backward_hidden_layer: init_value_backward,\n",
    "                    modelnn.forward_hidden_layer: init_value_forward,\n",
    "                },\n",
    "            )        \n",
    "            init_value_forward = last_state[0]\n",
    "            init_value_backward = last_state[1]\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value_forward = last_state[0]\n",
    "    init_value_backward = last_state[1]\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results8.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc8 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results8]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results8):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc8)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN_2p:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.BasicRNNCell(size_layer)\n",
    "        \n",
    "        with tf.variable_scope('forward', reuse = False):\n",
    "            rnn_cells_forward = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "                state_is_tuple = False,\n",
    "            )\n",
    "            self.X_forward = tf.placeholder(tf.float32, (None, None, size))\n",
    "            drop_forward = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_forward, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.hidden_layer_forward = tf.placeholder(\n",
    "                tf.float32, (None, num_layers * size_layer)\n",
    "            )\n",
    "            self.outputs_forward, self.last_state_forward = tf.nn.dynamic_rnn(\n",
    "                drop_forward,\n",
    "                self.X_forward,\n",
    "                initial_state = self.hidden_layer_forward,\n",
    "                dtype = tf.float32,\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('backward', reuse = False):\n",
    "            rnn_cells_backward = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "                state_is_tuple = False,\n",
    "            )\n",
    "            self.X_backward = tf.placeholder(tf.float32, (None, None, size))\n",
    "            drop_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_backward, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.hidden_layer_backward = tf.placeholder(\n",
    "                tf.float32, (None, num_layers * size_layer)\n",
    "            )\n",
    "            self.outputs_backward, self.last_state_backward = tf.nn.dynamic_rnn(\n",
    "                drop_backward,\n",
    "                self.X_backward,\n",
    "                initial_state = self.hidden_layer_backward,\n",
    "                dtype = tf.float32,\n",
    "            )\n",
    "\n",
    "        self.outputs = self.outputs_backward - self.outputs_forward\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results9 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = VanillaRNN_2p(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x_forward = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_x_backward = np.expand_dims(\n",
    "                np.flip(df_train.iloc[k : index, :].values, axis = 0), axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state_forward, last_state_backward, _, loss = sess.run(\n",
    "                [\n",
    "                    modelnn.logits,\n",
    "                    modelnn.last_state_forward,\n",
    "                    modelnn.last_state_backward,\n",
    "                    modelnn.optimizer,\n",
    "                    modelnn.cost,\n",
    "                ],\n",
    "                feed_dict = {\n",
    "                    modelnn.X_forward: batch_x_forward,\n",
    "                    modelnn.X_backward: batch_x_backward,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer_forward: init_value_forward,\n",
    "                    modelnn.hidden_layer_backward: init_value_backward,\n",
    "                },\n",
    "            )\n",
    "            init_value_forward = last_state_forward\n",
    "            init_value_backward = last_state_backward\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        batch_x_forward = np.expand_dims(\n",
    "        df_train.iloc[k : k + timestamp, :], axis = 0\n",
    "        )\n",
    "        batch_x_backward = np.expand_dims(\n",
    "            np.flip(df_train.iloc[k : k + timestamp, :].values, axis = 0), axis = 0\n",
    "        )\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [\n",
    "                modelnn.logits,\n",
    "                modelnn.last_state_forward,\n",
    "                modelnn.last_state_backward,\n",
    "            ],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: batch_x_forward,\n",
    "                modelnn.X_backward: batch_x_backward,\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[k + 1 : k + timestamp + 1, :] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        batch_x_forward = np.expand_dims(df_train.iloc[upper_b:, :], axis = 0)\n",
    "        batch_x_backward = np.expand_dims(\n",
    "            np.flip(df_train.iloc[upper_b:, :].values, axis = 0), axis = 0\n",
    "        )\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state_forward, modelnn.last_state_backward],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: batch_x_forward,\n",
    "                modelnn.X_backward: batch_x_backward,\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value_forward = last_state_forward\n",
    "    init_value_backward = last_state_backward\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        o_f = np.flip(o, axis = 0)\n",
    "        out_logits, last_state_forward, last_state_backward = sess.run(\n",
    "            [\n",
    "                modelnn.logits,\n",
    "                modelnn.last_state_forward,\n",
    "                modelnn.last_state_backward,\n",
    "            ],\n",
    "            feed_dict = {\n",
    "                modelnn.X_forward: np.expand_dims(o, axis = 0),\n",
    "                modelnn.X_backward: np.expand_dims(o_f, axis = 0),\n",
    "                modelnn.hidden_layer_forward: init_value_forward,\n",
    "                modelnn.hidden_layer_backward: init_value_backward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state_forward\n",
    "        init_value_backward = last_state_backward\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results9.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc9 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results9]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results9):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc9)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_seq2seq:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        _, last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        \n",
    "        with tf.variable_scope('decoder', reuse = False):\n",
    "            rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)], state_is_tuple = False\n",
    "            )\n",
    "            drop_dec = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_dec, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "                drop_dec, self.X, initial_state = last_state, dtype = tf.float32\n",
    "            )\n",
    "            \n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results10 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = LSTM_seq2seq(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer: init_value,\n",
    "                },\n",
    "            )        \n",
    "            init_value = last_state\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "    \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value = last_state\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results10.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc10 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results10]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results10):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_seq2seq_bid:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        backward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        forward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            backward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        forward_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            forward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.backward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        self.forward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        _, last_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_backward,\n",
    "            drop_backward,\n",
    "            self.X,\n",
    "            initial_state_fw = self.forward_hidden_layer,\n",
    "            initial_state_bw = self.backward_hidden_layer,\n",
    "            dtype = tf.float32,\n",
    "        )\n",
    "        \n",
    "        with tf.variable_scope('decoder', reuse = False):\n",
    "            backward_rnn_cells_decoder = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "            )\n",
    "            forward_rnn_cells_decoder = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "                state_is_tuple = False,\n",
    "            )\n",
    "            drop_backward_decoder = tf.contrib.rnn.DropoutWrapper(\n",
    "            backward_rnn_cells_decoder, output_keep_prob = forget_bias\n",
    "            )\n",
    "            forward_backward_decoder = tf.contrib.rnn.DropoutWrapper(\n",
    "                forward_rnn_cells_decoder, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.outputs, self.last_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                forward_backward_decoder, drop_backward_decoder, self.X, \n",
    "                initial_state_fw = last_state[0],\n",
    "                initial_state_bw = last_state[1],\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "        self.outputs = tf.concat(self.outputs, 2)\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results11 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = LSTM_seq2seq_bid(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value_forward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.backward_hidden_layer: init_value_backward,\n",
    "                    modelnn.forward_hidden_layer: init_value_forward,\n",
    "                },\n",
    "            )        \n",
    "            init_value_forward = last_state[0]\n",
    "            init_value_backward = last_state[1]\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "            \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * 2 * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "\n",
    "    init_value_forward = last_state[0]\n",
    "    init_value_backward = last_state[1]\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results11.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc11 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results11]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results11):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc11)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variational Auto-Encoder\n",
    "class LSTM_seq2seq_vae:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "        lambda_coeff = 0.5\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * 2 * size_layer)\n",
    "        )\n",
    "        _, last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        \n",
    "        self.z_mean = tf.layers.dense(last_state, size)\n",
    "        self.z_log_sigma = tf.layers.dense(last_state, size)\n",
    "        \n",
    "        epsilon = tf.random_normal(tf.shape(self.z_log_sigma))\n",
    "        self.z_vector = self.z_mean + tf.exp(self.z_log_sigma)\n",
    "        \n",
    "        with tf.variable_scope('decoder', reuse = False):\n",
    "            rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)], state_is_tuple = False\n",
    "            )\n",
    "            drop_dec = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_dec, output_keep_prob = forget_bias\n",
    "            )\n",
    "            x = tf.concat([tf.expand_dims(self.z_vector, axis=0), self.X], axis = 1)\n",
    "            self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "                drop_dec, self.X, initial_state = last_state, dtype = tf.float32\n",
    "            )\n",
    "            \n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "        \n",
    "        self.kl_loss = -0.5 * tf.reduce_sum(1.0 + 2 * self.z_log_sigma - self.z_mean ** 2 - \n",
    "                             tf.exp(2 * self.z_log_sigma), 1)\n",
    "        self.kl_loss = tf.scalar_mul(self.lambda_coeff, self.kl_loss)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results12 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = LSTM_seq2seq_vae(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_x = np.random.binomial(1, 0.5, batch_x.shape) * batch_x\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer: init_value,\n",
    "                },\n",
    "            )        \n",
    "            init_value = last_state\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "           \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * 2 * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "        \n",
    "    init_value = last_state\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "    \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results12.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc12 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results12]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results12):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc12)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_seq2seq:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.GRUCell(size_layer)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * size_layer)\n",
    "        )\n",
    "        _, last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        \n",
    "        with tf.variable_scope('decoder', reuse = False):\n",
    "            rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)], state_is_tuple = False\n",
    "            )\n",
    "            drop_dec = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_dec, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "                drop_dec, self.X, initial_state = last_state, dtype = tf.float32\n",
    "            )\n",
    "            \n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results13 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = GRU_seq2seq(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer: init_value,\n",
    "                },\n",
    "            )        \n",
    "            init_value = last_state\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "            \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "        \n",
    "    init_value = last_state\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "        \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results13.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc13 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results13]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results13):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc13)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_seq2seq_bid:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.GRUCell(size_layer)\n",
    "\n",
    "        backward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        forward_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            backward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        forward_backward = tf.contrib.rnn.DropoutWrapper(\n",
    "            forward_rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.backward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * size_layer)\n",
    "        )\n",
    "        self.forward_hidden_layer = tf.placeholder(\n",
    "            tf.float32, shape = (None, num_layers * size_layer)\n",
    "        )\n",
    "        _, last_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            forward_backward,\n",
    "            drop_backward,\n",
    "            self.X,\n",
    "            initial_state_fw = self.forward_hidden_layer,\n",
    "            initial_state_bw = self.backward_hidden_layer,\n",
    "            dtype = tf.float32,\n",
    "        )\n",
    "        \n",
    "        with tf.variable_scope('decoder', reuse = False):\n",
    "            backward_rnn_cells_decoder = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "            )\n",
    "            forward_rnn_cells_decoder = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "                state_is_tuple = False,\n",
    "            )\n",
    "            drop_backward_decoder = tf.contrib.rnn.DropoutWrapper(\n",
    "            backward_rnn_cells_decoder, output_keep_prob = forget_bias\n",
    "            )\n",
    "            forward_backward_decoder = tf.contrib.rnn.DropoutWrapper(\n",
    "                forward_rnn_cells_decoder, output_keep_prob = forget_bias\n",
    "            )\n",
    "            self.outputs, self.last_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                forward_backward_decoder, drop_backward_decoder, self.X, \n",
    "                initial_state_fw = last_state[0],\n",
    "                initial_state_bw = last_state[1],\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "        self.outputs = tf.concat(self.outputs, 2)\n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results14 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = GRU_seq2seq_bid(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "        init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.backward_hidden_layer: init_value_backward,\n",
    "                    modelnn.forward_hidden_layer: init_value_forward,\n",
    "                },\n",
    "            )        \n",
    "            init_value_forward = last_state[0]\n",
    "            init_value_backward = last_state[1]\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "            \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "\n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value_forward = np.zeros((1, num_layers * size_layer))\n",
    "    init_value_backward = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "        \n",
    "    init_value_forward = last_state[0]\n",
    "    init_value_backward = last_state[1]\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.backward_hidden_layer: init_value_backward,\n",
    "                modelnn.forward_hidden_layer: init_value_forward,\n",
    "            },\n",
    "        )\n",
    "        init_value_forward = last_state[0]\n",
    "        init_value_backward = last_state[1]\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "        \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results14.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc14 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results14]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results14):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc14)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU_seq2seq_vae:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate,\n",
    "        num_layers,\n",
    "        size,\n",
    "        size_layer,\n",
    "        output_size,\n",
    "        forget_bias = 0.1,\n",
    "        lambda_coeff = 0.5\n",
    "    ):\n",
    "        def lstm_cell(size_layer):\n",
    "            return tf.nn.rnn_cell.GRUCell(size_layer)\n",
    "\n",
    "        rnn_cells = tf.nn.rnn_cell.MultiRNNCell(\n",
    "            [lstm_cell(size_layer) for _ in range(num_layers)],\n",
    "            state_is_tuple = False,\n",
    "        )\n",
    "        self.X = tf.placeholder(tf.float32, (None, None, size))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, output_size))\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(\n",
    "            rnn_cells, output_keep_prob = forget_bias\n",
    "        )\n",
    "        self.hidden_layer = tf.placeholder(\n",
    "            tf.float32, (None, num_layers * size_layer)\n",
    "        )\n",
    "        _, last_state = tf.nn.dynamic_rnn(\n",
    "            drop, self.X, initial_state = self.hidden_layer, dtype = tf.float32\n",
    "        )\n",
    "        \n",
    "        self.z_mean = tf.layers.dense(last_state, size)\n",
    "        self.z_log_sigma = tf.layers.dense(last_state, size)\n",
    "        \n",
    "        epsilon = tf.random_normal(tf.shape(self.z_log_sigma))\n",
    "        self.z_vector = self.z_mean + tf.exp(self.z_log_sigma)\n",
    "        \n",
    "        with tf.variable_scope('decoder', reuse = False):\n",
    "            rnn_cells_dec = tf.nn.rnn_cell.MultiRNNCell(\n",
    "                [lstm_cell(size_layer) for _ in range(num_layers)], state_is_tuple = False\n",
    "            )\n",
    "            drop_dec = tf.contrib.rnn.DropoutWrapper(\n",
    "                rnn_cells_dec, output_keep_prob = forget_bias\n",
    "            )\n",
    "            x = tf.concat([tf.expand_dims(self.z_vector, axis=0), self.X], axis = 1)\n",
    "            self.outputs, self.last_state = tf.nn.dynamic_rnn(\n",
    "                drop_dec, self.X, initial_state = last_state, dtype = tf.float32\n",
    "            )\n",
    "            \n",
    "        self.logits = tf.layers.dense(self.outputs[-1], output_size)\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "        \n",
    "        self.kl_loss = -0.5 * tf.reduce_sum(1.0 + 2 * self.z_log_sigma - self.z_mean ** 2 - \n",
    "                             tf.exp(2 * self.z_log_sigma), 1)\n",
    "        self.kl_loss = tf.scalar_mul(self.lambda_coeff, self.kl_loss)\n",
    "        self.cost = tf.compat.v1.losses.mean_squared_error(self.Y, self.logits)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            self.cost\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results15 = []\n",
    "for i in range(simulation_size):\n",
    "    print('simulation %d'%(i + 1))\n",
    "    tf.reset_default_graph()\n",
    "    modelnn = GRU_seq2seq_vae(\n",
    "        learning_rate, num_layers, df_log.shape[1], size_layer, df_log.shape[1], dropout_rate\n",
    "    )\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    now = datetime.datetime.now()\n",
    "    \n",
    "    ## Training\n",
    "    for i in range(epoch):\n",
    "        init_value = np.zeros((1, num_layers * size_layer))\n",
    "        total_loss, total_acc = [], []\n",
    "        for k in range(0, df_train.shape[0] - 1, timestamp):\n",
    "            index = min(k + timestamp, df_train.shape[0] - 1)\n",
    "            batch_x = np.expand_dims(\n",
    "                df_train.iloc[k : index, :].values, axis = 0\n",
    "            )\n",
    "            batch_x = np.random.binomial(1, 0.5, batch_x.shape) * batch_x\n",
    "            batch_y = df_train.iloc[k + 1 : index + 1, :].values\n",
    "            logits, last_state, _, loss = sess.run(\n",
    "                [modelnn.logits, modelnn.last_state, modelnn.optimizer, modelnn.cost],\n",
    "                feed_dict = {\n",
    "                    modelnn.X: batch_x,\n",
    "                    modelnn.Y: batch_y,\n",
    "                    modelnn.hidden_layer: init_value,\n",
    "                },\n",
    "            )        \n",
    "            init_value = last_state\n",
    "            total_loss.append(loss)\n",
    "            total_acc.append(calculate_accuracy(batch_y[:, 0], logits[:, 0]))\n",
    "            \n",
    "    fin = datetime.datetime.now()\n",
    "    print(fin-now)\n",
    "    \n",
    "    ## Testing\n",
    "    future_day = test_size\n",
    "    \n",
    "    output_predict = np.zeros((df_train.shape[0] + future_day, df_train.shape[1]))\n",
    "    output_predict[0] = df_train.iloc[0]\n",
    "    upper_b = (df_train.shape[0] // timestamp) * timestamp\n",
    "    init_value = np.zeros((1, num_layers * size_layer))\n",
    "\n",
    "    for k in range(0, (df_train.shape[0] // timestamp) * timestamp, timestamp):\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(\n",
    "                    df_train.iloc[k : k + timestamp], axis = 0\n",
    "                ),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[k + 1 : k + timestamp + 1] = out_logits\n",
    "\n",
    "    if upper_b != df_train.shape[0]:\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(df_train.iloc[upper_b:], axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        output_predict[upper_b + 1 : df_train.shape[0] + 1] = out_logits\n",
    "        future_day -= 1\n",
    "        \n",
    "    init_value = last_state\n",
    "    \n",
    "    for i in range(future_day):\n",
    "        o = output_predict[-future_day - timestamp + i:-future_day + i]\n",
    "        out_logits, last_state = sess.run(\n",
    "            [modelnn.logits, modelnn.last_state],\n",
    "            feed_dict = {\n",
    "                modelnn.X: np.expand_dims(o, axis = 0),\n",
    "                modelnn.hidden_layer: init_value,\n",
    "            },\n",
    "        )\n",
    "        init_value = last_state\n",
    "        output_predict[-future_day + i] = out_logits[-1]\n",
    "        \n",
    "    output_predict = minmax.inverse_transform(output_predict)\n",
    "    deep_future = anchor(output_predict[:, 0], 0.3)\n",
    "    \n",
    "    results15.append(deep_future[-test_size:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc15 = [calculate_accuracy(df['4. close'].iloc[-test_size:].values, r) for r in results15]\n",
    "\n",
    "plt.figure(figsize = (15, 5))\n",
    "for no, r in enumerate(results15):\n",
    "    plt.plot(r, label = 'forecast %d'%(no + 1))\n",
    "    \n",
    "plt.plot(df['4. close'].iloc[-test_size:].values, label = 'true trend', c = 'black')\n",
    "plt.legend()\n",
    "plt.title('Average Accuracy: %.4f'%(np.mean(acc15)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array(results)\n",
    "a2 = np.array(results2)\n",
    "a3 = np.array(results3)\n",
    "a4 = np.array(results4)\n",
    "a5 = np.array(results5)\n",
    "a6 = np.array(results6)\n",
    "a7 = np.array(results7)\n",
    "a8 = np.array(results8)\n",
    "a9 = np.array(results9)\n",
    "a10 = np.array(results10)\n",
    "a11 = np.array(results11)\n",
    "a12 = np.array(results12)\n",
    "a13 = np.array(results13)\n",
    "a14 = np.array(results14)\n",
    "a15 = np.array(results15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('nflx_lstm.npy', a1)\n",
    "np.save('nflx_lstm_bid.npy', a2)\n",
    "np.save('nflx_lstm_2p.npy', a3)\n",
    "np.save('nflx_gru.npy', a4)\n",
    "np.save('nflx_gru_bid.npy', a5)\n",
    "np.save('nflx_gru_2p.npy', a6)\n",
    "np.save('nflx_vrnn.npy', a7)\n",
    "np.save('nflx_vrnn_bid.npy', a8)\n",
    "np.save('nflx_vrnn_2p.npy', a9)\n",
    "np.save('nflx_seq2seq.npy', a10)\n",
    "np.save('nflx_seq2seq_vae.npy', a12)\n",
    "np.save('nflx_seq2seq_bid.npy', a11)\n",
    "np.save('nflx_grubid_seq2seq.npy', a14)\n",
    "np.save('nflx_gru_seq2seq.npy', a13)\n",
    "np.save('nflx_gruvae_seq2seq.npy', a15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
